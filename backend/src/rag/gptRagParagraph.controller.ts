// backend\src\rag\gptRagParagraph.controller.ts
/*
  10.
  üí• ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒµŒØ œÑŒø prompt ŒºŒµ Œ≤Œ¨œÉŒ∑ œÑŒø context (semantic search) Œ∫Œ±Œπ Œ∫Œ±ŒªŒµŒØ œÑŒ∑ œÉœçŒΩŒ¥ŒµœÉŒ∑ ŒºŒµ œÑŒø OpenAI.

  prev ‚Üí backend\src\rag\gptRag.service.ts
  next ‚Üí backend\src\rag\gptRagParagraph.routes.ts (Œ∫Œ±Œπ ŒºŒµœÑŒ¨ frontend)
*/

import type { Request, Response } from 'express'
import dotenv from 'dotenv'
import { gptEmbeddingsService } from '../vectorize/gptEmbeddingsParagraph.service'
import { getGPTResponse } from './gptRag.service'
import { ParagraphType } from '../types/paragraph.types'
import axios from 'axios'

dotenv.config() // ŒºœÄŒøœÅŒµŒØ ŒΩŒ± Œ±œÜŒ±ŒπœÅŒµŒ∏ŒµŒØ, Œ±ŒªŒªŒ¨ Œ¥ŒµŒΩ œÄŒµŒπœÅŒ¨Œ∂ŒµŒπ Œ±ŒΩ ŒºŒµŒØŒΩŒµŒπ ‚Äî Œ¥ŒµŒΩ ŒµŒØŒΩŒ±Œπ standalone

// -------------------------------------------------------------
// POST /api/rag/ask
// -------------------------------------------------------------
// Œ±œÖœÑŒÆ ŒµŒ¥œé Œ±œÄŒø ŒªŒ¨Œ∏ŒøœÇ œÉœÑŒøŒΩ œÉœáŒµŒ¥ŒπŒ±œÉŒºœå Œ¥ŒµŒΩ Œ∫Œ¨ŒªŒµŒØ œÑŒ∑ŒΩ searchHandler œÑŒ∑œÇ backend\src\vectorize\gptEmbeddingsParagraph.controller.ts Œ∫Œ±Œπ ŒµœÜŒ±œÅŒºœåŒ∂ŒµŒπ Œ±œÄŒø œÑŒ∑ŒΩ Œ±œÅœáŒÆ ŒøŒªŒ∑ œÑŒ∑ŒΩ ŒªŒøŒ≥ŒπŒ∫ŒÆ œÑŒ∑œÇ. Œ¥ŒµŒΩ œÄŒµŒπœÅŒ¨Œ∂ŒµŒπ Œ∫Œ±Œπ Œ∏Œ± œÑŒø Œ±œÜŒÆœÉœâ. Œ±ŒªŒªŒ± œÉœÑŒ∑ŒΩ œÄŒ±œÅŒ±Œ∫Œ¨œÑœâ Œ∏Œ± Œ¥ŒπŒøœÅŒ∏œâŒ∏ŒµŒØ
const askWithContext = async (req: Request, res: Response) => {
  try {
    // Œ∑ ŒµœÅœéœÑŒ∑œÉŒ∑ string Œ±œÄœå œÑŒø frontend
    // Œ∫Œ±Œπ œÄœÅŒøŒ±ŒπœÅŒµœÑŒπŒ∫Œ¨ Œ≠ŒΩŒ± ŒºŒπŒ∫œÅœå ŒπœÉœÑŒøœÅŒπŒ∫œå œÑœâŒΩ œÑŒµŒªŒµœÖœÑŒ±ŒØœâŒΩ 4 ŒµœÅœâœÑŒøŒ±œÄŒ±ŒΩœÑŒÆœÉŒµœâŒΩ
    const { query, history } = req.body

    if (!query || typeof query !== 'string') {
      return res.status(400).json({ status: false, message: 'Missing query text' })
    }

    // Œ¥ŒπŒ±ŒºœåœÅœÜœâœÉŒ∑ ŒπœÉœÑŒøœÅŒπŒ∫Œøœç (ŒªŒØŒ≥Œ∑ ‚ÄúŒºŒΩŒÆŒºŒ∑‚Äù Œ≥ŒπŒ± œÑŒø chat)
    const chatHistory = Array.isArray(history)
      ? history
          .map((h: { role: string; content: string }) =>
            `${h.role === 'user' ? 'User' : 'Assistant'}: ${h.content}`
          )
          .join('\n')
      : ''

    const apiKey = process.env.OPENAI_API_KEY
    if (!apiKey) {
      return res.status(500).json({ status: false, message: 'OPENAI_API_KEY not set' })
    }

    // 1Ô∏è‚É£ semantic search ‚Äî Œ≤œÅŒØœÉŒ∫ŒøœÖŒºŒµ œÑŒπœÇ 5 œÄŒπŒø Œ∫ŒøŒΩœÑŒπŒΩŒ≠œÇ œÄŒ±œÅŒ±Œ≥œÅŒ¨œÜŒøœÖœÇ
    // Œ±œÖœÑŒÆ ŒµŒØŒΩŒ±Œπ Œ∑ œÄŒ±ŒªŒπŒ¨ "œáŒµŒπœÅŒøŒ∫ŒØŒΩŒ∑œÑŒ∑" ŒºŒµŒ∏ŒøŒ¥ŒøœÇ œÄŒøœÖ ŒÆœÑŒ±ŒΩ œÄŒøŒªœç Œ±œÅŒ≥ŒÆ Œ∫Œ±Œπ œÑŒ∑ŒΩ Œ±ŒΩœÑŒπŒ∫Œ±œÑŒ±œÉœÑŒÆœÉŒ±ŒºŒµ ŒºŒµ œÑŒ∑ŒΩ semanticSearchParagraphsVector
    // const topParagraphs = await gptEmbeddingsService.semanticSearchParagraphs(query, 5)

    let topParagraphs: ParagraphType[] = []
    try {
      console.log('using mongo vector search');
      // fast Mongo vector search
      topParagraphs = await gptEmbeddingsService.semanticSearchParagraphsVector(query, 5)
    } catch (err) {
      // old search
      console.error('‚ùå Vector search failed:', JSON.stringify(err, null, 2))
      console.warn('‚ö†Ô∏è Vector search unavailable, falling back to JS cosine search')
      topParagraphs = await gptEmbeddingsService.semanticSearchParagraphs(query, 5)
    }


    // 2Ô∏è‚É£ Œ¥Œ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± context string
    const context = topParagraphs
      .map(
        (p: { paragraphNumber?: number | string | null; text?: string | null }, i: number) =>
          `Excerpt ${i + 1} (Paragraph ${p.paragraphNumber ?? '?'}):\n${p.text?.trim() ?? ''}`
      )
      .join('\n\n')

    // 3Ô∏è‚É£ œÉœçŒΩŒ∏ŒµœÉŒ∑ œÑŒøœÖ œÄŒªŒÆœÅŒøœÖœÇ prompt (œÉœçŒºœÜœâŒΩŒ± ŒºŒµ œÑŒ∑ ŒªŒøŒ≥ŒπŒ∫ŒÆ RAG)
    const prompt = `
      You are a scholarly assistant specializing in Karl Marx‚Äôs *Das Kapital*.
      
      Recent conversation:
      ${chatHistory || '(no previous context)'}

      Use the following excerpts from *Capital* as factual context to answer the user's question.
      Stay faithful to Marx‚Äôs terminology and reasoning.
      Do not invent new ideas ‚Äî base your response strictly on the provided text.

      Context:
      ${context}

      Question:
      ${query}

      Answer:
    `.trim()

    // 4Ô∏è‚É£ Œ∫Œ±ŒªŒøœçŒºŒµ œÑŒø OpenAI API
    const gptAnswer = await getGPTResponse(prompt, apiKey)

    // 5Ô∏è‚É£ ŒµœÄŒπœÉœÑœÅŒ≠œÜŒøœÖŒºŒµ JSON Œ±œÄŒ¨ŒΩœÑŒ∑œÉŒ∑
    // 5Ô∏è‚É£ ŒµœÄŒπœÉœÑœÅŒ≠œÜŒøœÖŒºŒµ JSON Œ±œÄŒ¨ŒΩœÑŒ∑œÉŒ∑ œáœâœÅŒØœÇ œÑŒ± vectors
    return res.json({
      status: true,
      question: query,
      answer: gptAnswer,
      context: topParagraphs.map(p => ({
        _id: p._id,
        book: p.book,
        chapter: p.chapter,
        chapterTitle: p.chapterTitle,
        sectionTitle: p.sectionTitle,
        subsectionTitle: p.subsectionTitle,
        subsubsectionTitle: p.subsubsectionTitle,
        paragraphNumber: p.paragraphNumber,
        text: p.text,
        score: p.score
      }))
    })

  } catch (err: unknown) {
    const msg = err instanceof Error ? err.message : 'Unknown error'
    return res.status(500).json({ status: false, message: msg })
  }
}

// -------------------------------------------------------------
// POST /api/rag/ask-extended
// -------------------------------------------------------------
const askWithContextExtended = async (req: Request, res: Response) => {
  try {
    const { query, history } = req.body

    if (!query || typeof query !== 'string') {
      return res.status(400).json({ status: false, message: 'Missing query text' })
    }

    // Œ¥ŒπŒ±ŒºœåœÅœÜœâœÉŒ∑ ŒπœÉœÑŒøœÅŒπŒ∫Œøœç
    const chatHistory = Array.isArray(history)
      ? history.map((h: { role: string; content: string }) =>
          `${h.role === 'user' ? 'User' : 'Assistant'}: ${h.content}`
        ).join('\n')
      : ''

    const apiKey = process.env.OPENAI_API_KEY
    if (!apiKey) {
      return res.status(500).json({ status: false, message: 'OPENAI_API_KEY not set' })
    }

    console.log('üß© Fetching extended semantic context (¬±3 paragraphs)...')

    // üîπ 1Ô∏è‚É£ Internal HTTP call œÉœÑŒø /api/vectorise/search-extended
    const backendUrl = process.env.BACKEND_URL || 'http://localhost:3001'
    const response = await axios.post(`${backendUrl}/api/vectorise/search-some-extended`, { query })

    // œÄŒ±ŒØœÅŒΩŒøœÖŒºŒµ ŒºœåŒΩŒø œÑŒø data array
    const extendedResults = response.data.data

    // Œ±œÄŒø ŒµŒ¥œé Œ∫Œ±Œπ œÄŒµœÅŒ± ŒµŒØŒΩŒ±Œπ ŒØŒ¥Œ± ŒºŒµ œÑŒ∑ŒΩ Œ±œÄŒø œÄŒ¨ŒΩœâ Œ±ŒªŒªŒ± Œ¥ŒµŒΩ œÄŒµŒπœÅŒ¨Œ∂ŒµŒπ
    if (!extendedResults || !Array.isArray(extendedResults)) {
      return res.status(500).json({ status: false, message: 'No extended context found' })
    }

    // üîπ 2Ô∏è‚É£ Œ¶œÑŒπŒ¨œáŒΩŒøœÖŒºŒµ context string
    const context = extendedResults
      .map(
        (r: any, i: number) =>
          `Excerpt ${i + 1} (Paragraph ${r.paragraphNumber ?? '?'}):\n${r.mergedText ?? ''}`
      )
      .join('\n\n')

    // üîπ 3Ô∏è‚É£ Prompt
    const prompt = `
      You are a scholarly assistant specializing in Karl Marx‚Äôs *Das Kapital*.

      Recent conversation:
      ${chatHistory || '(no previous context)'}

      Use the following extended excerpts (each ¬±3 paragraphs) as factual context.
      Stay faithful to Marx‚Äôs terminology and reasoning.
      Avoid speculation; rely only on the given context.

      Context:
      ${context}

      Question:
      ${query}

      Answer:
    `.trim()

    console.log('üß† Sending RAG prompt to OpenAI...')

    // üîπ 4Ô∏è‚É£ GPT answer
    const gptAnswer = await getGPTResponse(prompt, apiKey)

    // üîπ 5Ô∏è‚É£ Return
    return res.json({
      status: true,
      question: query,
      answer: gptAnswer,
      context: extendedResults
    })
  } catch (err: unknown) {
    const msg = err instanceof Error ? err.message : 'Unknown error'
    return res.status(500).json({ status: false, message: msg })
  }
}

// -------------------------------------------------------------
// export
// -------------------------------------------------------------
export const gptRagParagraphController = {
  askWithContext,
  askWithContextExtended
}
