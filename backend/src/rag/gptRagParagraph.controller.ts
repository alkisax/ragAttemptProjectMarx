// backend\src\rag\gptRagParagraph.controller.ts
/*
  10.
  ðŸ’¥ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï„Î¿ prompt Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ context (semantic search) ÎºÎ±Î¹ ÎºÎ±Î»ÎµÎ¯ Ï„Î· ÏƒÏÎ½Î´ÎµÏƒÎ· Î¼Îµ Ï„Î¿ OpenAI.

  prev â†’ backend\src\rag\gptRag.service.ts
  next â†’ backend\src\rag\gptRagParagraph.routes.ts (ÎºÎ±Î¹ Î¼ÎµÏ„Î¬ frontend)
*/

import type { Request, Response } from 'express'
import dotenv from 'dotenv'
import { gptEmbeddingsService } from '../vectorize/gptEmbeddingsParagraph.service'
import { getGPTResponse } from './gptRag.service'
import { ParagraphType } from '../types/paragraph.types'
import axios from 'axios'

dotenv.config() // Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î±Ï†Î±Î¹ÏÎµÎ¸ÎµÎ¯, Î±Î»Î»Î¬ Î´ÎµÎ½ Ï€ÎµÎ¹ÏÎ¬Î¶ÎµÎ¹ Î±Î½ Î¼ÎµÎ¯Î½ÎµÎ¹ â€” Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ standalone

// -------------------------------------------------------------
// POST /api/rag/ask
// -------------------------------------------------------------
// Î±Ï…Ï„Î® ÎµÎ´ÏŽ Î±Ï€Î¿ Î»Î¬Î¸Î¿Ï‚ ÏƒÏ„Î¿Î½ ÏƒÏ‡ÎµÎ´Î¹Î±ÏƒÎ¼ÏŒ Î´ÎµÎ½ ÎºÎ¬Î»ÎµÎ¯ Ï„Î·Î½ searchHandler Ï„Î·Ï‚ backend\src\vectorize\gptEmbeddingsParagraph.controller.ts ÎºÎ±Î¹ ÎµÏ†Î±ÏÎ¼ÏŒÎ¶ÎµÎ¹ Î±Ï€Î¿ Ï„Î·Î½ Î±ÏÏ‡Î® Î¿Î»Î· Ï„Î·Î½ Î»Î¿Î³Î¹ÎºÎ® Ï„Î·Ï‚. Î´ÎµÎ½ Ï€ÎµÎ¹ÏÎ¬Î¶ÎµÎ¹ ÎºÎ±Î¹ Î¸Î± Ï„Î¿ Î±Ï†Î®ÏƒÏ‰. Î±Î»Î»Î± ÏƒÏ„Î·Î½ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Î¸Î± Î´Î¹Î¿ÏÎ¸Ï‰Î¸ÎµÎ¯
const askWithContext = async (req: Request, res: Response) => {
  try {
    // Î· ÎµÏÏŽÏ„Î·ÏƒÎ· string Î±Ï€ÏŒ Ï„Î¿ frontend
    // ÎºÎ±Î¹ Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬ Î­Î½Î± Î¼Î¹ÎºÏÏŒ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ Ï„Ï‰Î½ Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Ï‰Î½ 4 ÎµÏÏ‰Ï„Î¿Î±Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½
    const { query, history, pastDiscussionSummary } = req.body

    if (!query || typeof query !== 'string') {
      return res.status(400).json({ status: false, message: 'Missing query text' })
    }

    // Î´Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ· Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï (Î»Î¯Î³Î· â€œÎ¼Î½Î®Î¼Î·â€ Î³Î¹Î± Ï„Î¿ chat)
    const chatHistory = Array.isArray(history)
      ? history
          .map((h: { role: string; content: string }) =>
            `${h.role === 'user' ? 'User' : 'Assistant'}: ${h.content}`
          )
          .join('\n')
      : ''

    const pastBlock = pastDiscussionSummary?.trim() || '(none yet)'

    const apiKey = process.env.OPENAI_API_KEY
    if (!apiKey) {
      return res.status(500).json({ status: false, message: 'OPENAI_API_KEY not set' })
    }

    // 1ï¸âƒ£ semantic search â€” Î²ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ Ï„Î¹Ï‚ 5 Ï€Î¹Î¿ ÎºÎ¿Î½Ï„Î¹Î½Î­Ï‚ Ï€Î±ÏÎ±Î³ÏÎ¬Ï†Î¿Ï…Ï‚
    // Î±Ï…Ï„Î® ÎµÎ¯Î½Î±Î¹ Î· Ï€Î±Î»Î¹Î¬ "Ï‡ÎµÎ¹ÏÎ¿ÎºÎ¯Î½Î·Ï„Î·" Î¼ÎµÎ¸Î¿Î´Î¿Ï‚ Ï€Î¿Ï… Î®Ï„Î±Î½ Ï€Î¿Î»Ï Î±ÏÎ³Î® ÎºÎ±Î¹ Ï„Î·Î½ Î±Î½Ï„Î¹ÎºÎ±Ï„Î±ÏƒÏ„Î®ÏƒÎ±Î¼Îµ Î¼Îµ Ï„Î·Î½ semanticSearchParagraphsVector
    // const topParagraphs = await gptEmbeddingsService.semanticSearchParagraphs(query, 5)

    let topParagraphs: ParagraphType[] = []
    try {
      console.log('using mongo vector search');
      // fast Mongo vector search
      topParagraphs = await gptEmbeddingsService.semanticSearchParagraphsVector(query, 5)
    } catch (err) {
      // old search
      console.error('âŒ Vector search failed:', JSON.stringify(err, null, 2))
      console.warn('âš ï¸ Vector search unavailable, falling back to JS cosine search')
      topParagraphs = await gptEmbeddingsService.semanticSearchParagraphs(query, 5)
    }


    // 2ï¸âƒ£ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± context string
    const context = topParagraphs
      .map(
        (p: { paragraphNumber?: number | string | null; text?: string | null }, i: number) =>
          `Excerpt ${i + 1} (Paragraph ${p.paragraphNumber ?? '?'}):\n${p.text?.trim() ?? ''}`
      )
      .join('\n\n')

    // 3ï¸âƒ£ ÏƒÏÎ½Î¸ÎµÏƒÎ· Ï„Î¿Ï… Ï€Î»Î®ÏÎ¿Ï…Ï‚ prompt (ÏƒÏÎ¼Ï†Ï‰Î½Î± Î¼Îµ Ï„Î· Î»Î¿Î³Î¹ÎºÎ® RAG)
    const prompt = `
      You are a scholarly assistant specializing in Karl Marxâ€™s *Das Kapital*.
      
      Recent conversation:
      ${chatHistory || '(no previous context)'}

      Cumulative summary of earlier discussions:
      ${pastBlock}

      Use the following excerpts from *Capital* as factual context to answer the user's question.
      Stay faithful to Marxâ€™s terminology and reasoning.
      Do not invent new ideas â€” base your response strictly on the provided text.

      Context:
      ${context}

      Question:
      ${query}

      Answer:
    `.trim()

    // 4ï¸âƒ£ ÎºÎ±Î»Î¿ÏÎ¼Îµ Ï„Î¿ OpenAI API
    const gptAnswer = await getGPTResponse(prompt, apiKey)

    // 5ï¸âƒ£ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ JSON Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·
    // 5ï¸âƒ£ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ JSON Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· Ï‡Ï‰ÏÎ¯Ï‚ Ï„Î± vectors
    return res.json({
      status: true,
      question: query,
      answer: gptAnswer,
      context: topParagraphs.map(p => ({
        _id: p._id,
        book: p.book,
        chapter: p.chapter,
        chapterTitle: p.chapterTitle,
        sectionTitle: p.sectionTitle,
        subsectionTitle: p.subsectionTitle,
        subsubsectionTitle: p.subsubsectionTitle,
        paragraphNumber: p.paragraphNumber,
        text: p.text,
        score: p.score
      }))
    })

  } catch (err: unknown) {
    const msg = err instanceof Error ? err.message : 'Unknown error'
    return res.status(500).json({ status: false, message: msg })
  }
}

// -------------------------------------------------------------
// POST /api/rag/ask-extended
// -------------------------------------------------------------
const askWithContextExtended = async (req: Request, res: Response) => {
  try {
    const { query, history, pastDiscussionSummary } = req.body

    if (!query || typeof query !== 'string') {
      return res.status(400).json({ status: false, message: 'Missing query text' })
    }

    // Î´Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ· Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï
    const chatHistory = Array.isArray(history)
      ? history.map((h: { role: string; content: string }) =>
          `${h.role === 'user' ? 'User' : 'Assistant'}: ${h.content}`
        ).join('\n')
      : ''

    const pastBlock = pastDiscussionSummary?.trim() || '(none yet)'

    const apiKey = process.env.OPENAI_API_KEY
    if (!apiKey) {
      return res.status(500).json({ status: false, message: 'OPENAI_API_KEY not set' })
    }

    console.log('ðŸ§© Fetching extended semantic context (Â±3 paragraphs)...')

    // ðŸ”¹ 1ï¸âƒ£ Internal HTTP call ÏƒÏ„Î¿ /api/vectorise/search-extended
    const backendUrl = process.env.BACKEND_URL || 'http://localhost:3001'
    const response = await axios.post(`${backendUrl}/api/vectorise/search-some-extended`, { query })

    // Ï€Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Î¼ÏŒÎ½Î¿ Ï„Î¿ data array
    const extendedResults = response.data.data

    // Î±Ï€Î¿ ÎµÎ´ÏŽ ÎºÎ±Î¹ Ï€ÎµÏÎ± ÎµÎ¯Î½Î±Î¹ Î¯Î´Î± Î¼Îµ Ï„Î·Î½ Î±Ï€Î¿ Ï€Î¬Î½Ï‰ Î±Î»Î»Î± Î´ÎµÎ½ Ï€ÎµÎ¹ÏÎ¬Î¶ÎµÎ¹
    if (!extendedResults || !Array.isArray(extendedResults)) {
      return res.status(500).json({ status: false, message: 'No extended context found' })
    }

    // ðŸ”¹ 2ï¸âƒ£ Î¦Ï„Î¹Î¬Ï‡Î½Î¿Ï…Î¼Îµ context string
    const context = extendedResults
      .map(
        (r: any, i: number) =>
          `Excerpt ${i + 1} (Paragraph ${r.paragraphNumber ?? '?'}):\n${r.mergedText ?? ''}`
      )
      .join('\n\n')

    // ðŸ”¹ 3ï¸âƒ£ Prompt
    const prompt = `
      You are a scholarly assistant specializing in Karl Marxâ€™s *Das Kapital*.

      Recent conversation:
      ${chatHistory || '(no previous context)'}

      Cumulative summary of earlier discussions:
      ${pastBlock}

      Use the following extended excerpts (each Â±3 paragraphs) as factual context.
      Stay faithful to Marxâ€™s terminology and reasoning.
      Avoid speculation; rely only on the given context.

      Context:
      ${context}

      Question:
      ${query}

      Answer:
    `.trim()

    console.log('ðŸ§  Sending RAG prompt to OpenAI...')

    // ðŸ”¹ 4ï¸âƒ£ GPT answer
    const gptAnswer = await getGPTResponse(prompt, apiKey)

    // ðŸ”¹ 5ï¸âƒ£ Return
    return res.json({
      status: true,
      question: query,
      answer: gptAnswer,
      context: extendedResults
    })
  } catch (err: unknown) {
    const msg = err instanceof Error ? err.message : 'Unknown error'
    return res.status(500).json({ status: false, message: msg })
  }
}

// ðŸ’£ðŸ’£ 14 ðŸ’¥ðŸ’¥Î±Ï…Ï„ÏŒ Ï€ÏÎ¿ÏƒÏ„Î­Î¸Î·ÎºÎµ Î¼ÎµÏ„Î¬ Ï„Î¿ Î²Î®Î¼Î± 13 Î¿ ÏƒÎºÎ¿Ï€ÏŒÏ‚ ÎµÎ¯Î½Î±Î¹ Î½Î± Ï†Ï„Î¹Î±Ï‡Ï„ÎµÎ¯ index Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Ï‰ÏƒÏ„Îµ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ hybrid search Î¼Îµ semantic ÎºÎ±Î¹ Î¼Îµ BM25 (text-based)
// Î¿Î¹ Î´ÏÎ¿ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ functions Î­Î¹Î½Î±Î¹ Î¿Ï…ÏƒÎ¹Î±ÏƒÏ„Î¹ÎºÎ¬ Î¯Î´Î¹ÎµÏ‚ Î¼Îµ Ï„Î¹Ï‚ Î±Ï€Î¿ Ï€Î±Î½Ï‰ Î¼ÏŒÎ½Î¿ Ï€Î¿Ï… Î±Î½Ï„Î¹ Î½Î± ÎºÎ±Î»Î¿Ï…Î½ Ï„Î± ${backendUrl}/api/vectorise/search-some-extended ÎºÎ±Î»Î¿ÏÎ½ Ï„Î± Î±Î½Ï„Î¯ÏƒÏ„Î¿Î¹Ï‡Î± hybrid search endpoints

// -------------------------------------------------------------
// POST /api/rag/ask-hybrid
// -------------------------------------------------------------
const askWithContextHybrid = async (req: Request, res: Response) => {
  try {
    const { query, history, pastDiscussionSummary } = req.body

    if (!query || typeof query !== 'string') {
      return res.status(400).json({ status: false, message: 'Missing query text' })
    }

    // 1ï¸âƒ£ Ï€ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï ÎºÎ±Î¹ Ï€ÎµÏÎ¯Î»Î·ÏˆÎ·Ï‚
    const chatHistory = Array.isArray(history)
      ? history.map((h: { role: string; content: string }) =>
          `${h.role === 'user' ? 'User' : 'Assistant'}: ${h.content}`
        ).join('\n')
      : ''

    const pastBlock = pastDiscussionSummary?.trim() || '(none yet)'
    const apiKey = process.env.OPENAI_API_KEY
    const backendUrl = process.env.BACKEND_URL || 'http://localhost:3001'

    if (!apiKey) return res.status(500).json({ status: false, message: 'OPENAI_API_KEY not set' })

    // 2ï¸âƒ£ Hybrid search (BM25 + vector)
    console.log('âš™ï¸ Running hybrid search...')
    const response = await axios.post(`${backendUrl}/api/vectorise/search-hybrid`, { query })
    const hybridResults = response.data.data

    if (!hybridResults || !Array.isArray(hybridResults)) {
      return res.status(500).json({ status: false, message: 'No hybrid context found' })
    }

    // 3ï¸âƒ£ Context block
    const context = hybridResults
      .map(
        (r: any, i: number) =>
          `Excerpt ${i + 1} (Paragraph ${r.paragraphNumber ?? '?'}):\n${r.text ?? ''}`
      )
      .join('\n\n')

    // 4ï¸âƒ£ Prompt
    const prompt = `
      You are a scholarly assistant specializing in Karl Marxâ€™s *Das Kapital*.

      Recent conversation:
      ${chatHistory || '(no previous context)'}

      Cumulative summary of earlier discussions:
      ${pastBlock}

      Use the following hybrid search results (semantic + text relevance)
      as factual context. Stay faithful to Marxâ€™s terminology and reasoning.

      Context:
      ${context}

      Question:
      ${query}

      Answer:
    `.trim()

    console.log('ðŸ§  Sending hybrid RAG prompt to OpenAI...')
    const gptAnswer = await getGPTResponse(prompt, apiKey)

    return res.json({
      status: true,
      question: query,
      answer: gptAnswer,
      context: hybridResults
    })
  } catch (err: unknown) {
    const msg = err instanceof Error ? err.message : 'Unknown error'
    return res.status(500).json({ status: false, message: msg })
  }
}

// -------------------------------------------------------------
// POST /api/rag/ask-extended-hybrid
// -------------------------------------------------------------
const askWithContextExtendedHybrid = async (req: Request, res: Response) => {
  try {
    const { query, history, pastDiscussionSummary } = req.body

    if (!query || typeof query !== 'string') {
      return res.status(400).json({ status: false, message: 'Missing query text' })
    }

    const chatHistory = Array.isArray(history)
      ? history.map((h: { role: string; content: string }) =>
          `${h.role === 'user' ? 'User' : 'Assistant'}: ${h.content}`
        ).join('\n')
      : ''

    const pastBlock = pastDiscussionSummary?.trim() || '(none yet)'
    const apiKey = process.env.OPENAI_API_KEY
    const backendUrl = process.env.BACKEND_URL || 'http://localhost:3001'

    if (!apiKey) return res.status(500).json({ status: false, message: 'OPENAI_API_KEY not set' })

    console.log('ðŸ§© Fetching extended hybrid context (Â±2 paragraphs)...')
    const response = await axios.post(`${backendUrl}/api/vectorise/search-some-extended-hybrid`, { query })
    const extendedHybridResults = response.data.data

    if (!extendedHybridResults || !Array.isArray(extendedHybridResults)) {
      return res.status(500).json({ status: false, message: 'No extended hybrid context found' })
    }

    const context = extendedHybridResults
      .map(
        (r: any, i: number) =>
          `Excerpt ${i + 1} (Paragraph ${r.paragraphNumber ?? '?'}):\n${r.mergedText ?? ''}`
      )
      .join('\n\n')

    const prompt = `
      You are a scholarly assistant specializing in Karl Marxâ€™s *Das Kapital*.

      Recent conversation:
      ${chatHistory || '(no previous context)'}

      Cumulative summary of earlier discussions:
      ${pastBlock}

      Use the following hybrid results (BM25 + vector) with Â±2 paragraph context.
      Stay faithful to Marxâ€™s terminology and reasoning.

      Context:
      ${context}

      Question:
      ${query}

      Answer:
    `.trim()

    console.log('ðŸ§  Sending hybrid-extended RAG prompt to OpenAI...')
    const gptAnswer = await getGPTResponse(prompt, apiKey)

    return res.json({
      status: true,
      question: query,
      answer: gptAnswer,
      context: extendedHybridResults
    })
  } catch (err: unknown) {
    const msg = err instanceof Error ? err.message : 'Unknown error'
    return res.status(500).json({ status: false, message: msg })
  }
}

// -------------------------------------------------------------
// export
// -------------------------------------------------------------
export const gptRagParagraphController = {
  askWithContext,
  askWithContextExtended,
  askWithContextHybrid,
  askWithContextExtendedHybrid
}
